{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yFFKuDZB_ow"
      },
      "source": [
        "# Prompt Enginnering\n",
        "### **Course Name:** Natural Language Processing **<font color=\"red\">(CSC4100)</font>**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_qFy6J9EgJe"
      },
      "source": [
        "Hello, everyone.\n",
        "In this tutorial, we'll explore *Prompt Engineering* techniques.\n",
        "\n",
        "**<font color=\"blue\">What is prompt engineering in AI?</font>**\n",
        "\n",
        "An AI prompt is a carefully crafted instruction given to an AI model to generate a specific output. These inputs can range from text and images to videos or even music.\n",
        "\n",
        "Prompt engineering means writing precise instructions that guide AI models like **ChatGPT** to produce specific and useful responses. It involves designing inputs that an AI can easily understand and act upon, ensuring the output is relevant and accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEiLphcKt6Ya"
      },
      "source": [
        "## **The First Step: Mastering the Use of APIs**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR37y8eVuG8V"
      },
      "source": [
        "### About API Keys\n",
        "#### DeepSeek API Keys\n",
        "‚ùó Note that you can go [Deepseek](https://platform.deepseek.com/api_keys) to get your api key. Alternatively,  youbcan go to [Siliconflow](https://cloud.siliconflow.cn/) for free credit:\n",
        "- Go to the website [https://platform.deepseek.com/api_keys](https://platform.deepseek.com/api_keys).\n",
        "- Setup your api key through setting the environment variable os.environ[\"DEEPSEEK_API_KEY\"]\n",
        "- Remember to update `DEEPSEEK_BASE_URL` to https://api.siliconflow.cn/v1/chat/completions when using API from SiliconFlow.\n",
        "\n",
        "#### OpenAI API Keys\n",
        "Note that we provide a key with 100 US dollars, if it is used up you need to buy the Keys yourself (it may cost you a little bit of money), here is how to buy the keys:\n",
        "- Go to the website [https://eylink.cn/buy/7](https://eylink.cn/buy/7).\n",
        "- Purchase a 14 RMB key (10 US dollars). (10 dollars are enough.)\n",
        "- Fill in the `OPENAI_API_KEY` below with the key you purchased.\n",
        "\n",
        "(As a student, you can apply for a $100 free API credit at https://azure.microsoft.com/en-us/free/students. Remember to update `OPENAI_BASE_URL` to https://api.openai.com/v1/chat/completions when using API from Azure.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOubmZd8vGl6"
      },
      "source": [
        "üîÖ To facilitate easier access to OpenAI's model APIs, we make use of a popular framework langchain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OWx2mmC-fFuP",
        "outputId": "3eb602c9-5117-4da8-a432-a3ed9fb32f9a"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-deepseek\n",
        "!pip install retrying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "EXIC-LVvVXit"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import random\n",
        "import json\n",
        "from retrying import retry\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DeepSeek Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an AI assistant, please answer user's question.\"),\n",
        "        (\"user\", \"{input}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set DeepSeek API key and base URL\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-\"\n",
        "os.environ[\"DEEPSEEK_BASE_URL\"] = \"https://api.deepseek.com/v1\"\n",
        "deepseek_chat = ChatDeepSeek(model=\"deepseek-chat\", temperature=1)\n",
        "\n",
        "model = ChatDeepSeek(model=\"deepseek-chat\", temperature=1)\n",
        "\n",
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ElnWZGKu9Vo"
      },
      "source": [
        "üòä You can now engage directly with DeepSeek-Chat using our `invoke` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzUZFHrweW2D",
        "outputId": "346bcdf2-e4c8-40c1-f746-f7c456dfc08d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Hello! How can I help you today? üòä' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 17, 'total_tokens': 28, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 17}, 'model_provider': 'deepseek', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'aa08df44-143f-4701-a54b-91d4f958c2df', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--43875c1d-9c35-4ef6-9f36-5044d107a499-0' usage_metadata={'input_tokens': 17, 'output_tokens': 11, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\"input\": \"Hello\"})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt-3.5-turbo-0125\n",
            "gpt-4\n",
            "gpt-4-0613\n",
            "gpt-4-turbo-2024-04-09\n",
            "gpt-4o-2024-05-13\n",
            "gpt-4o-mini-2024-07-18\n",
            "gpt-4o-2024-08-06\n",
            "gpt-5-2025-08-07\n",
            "o1-2024-12-17\n",
            "gpt-5-mini-2025-08-07\n",
            "gpt-5-nano-2025-08-07\n",
            "gpt-4.1-2025-04-14\n",
            "gpt-4.1-nano-2025-04-14\n",
            "gpt-4o-2024-11-20\n",
            "o3-mini-2025-01-31\n",
            "gpt-5-chat-2025-08-07\n",
            "o4-mini-2025-04-16\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"sk-\", base_url=\"http://66.206.9.230:4000/v1\")\n",
        "\n",
        "models = client.models.list()\n",
        "for m in models.data:\n",
        "    print(m.id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model_4o_mini = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini-2024-07-18\",\n",
        "    temperature=1,\n",
        "    openai_api_key=\"sk-\",\n",
        "    openai_api_base=\"http://66.206.9.230:4000/v1\"   # explicit argument\n",
        ")\n",
        "\n",
        "chain = prompt | model_4o_mini \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 23, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-CXl7ICdUErXa1neL08P9LuSafQJq6', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--6eae65d4-076d-4528-9cf0-0bbc826483fa-0' usage_metadata={'input_tokens': 23, 'output_tokens': 10, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\"input\": \"Hello\"})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ly8ERAJgI8m"
      },
      "source": [
        "To make the output easy to use, we can apply a output parser to the original output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPnmOckZgICw",
        "outputId": "4d752498-82c7-4d49-ca1c-debdda626176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "response = chain.invoke({\"input\": \"Hello\"})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADk-FqFZvaNn"
      },
      "source": [
        "### **LLM Setting**\n",
        "\n",
        "**1. Model Selection**\n",
        "\n",
        "You are free to choose from several models. Below are the models DeepSeek offers:\n",
        "- deepseek-chat (currently pointing to deepseek-v3, offering a balance of capability and cost, highly recommended!)\n",
        "- deepseek-reasoner (currently pointing to deepseek-r1, extremely sophisticated and intelligent)\n",
        "For detailed information, please visit [DeepSeek's Website](https://api-docs.deepseek.com/zh-cn/news/news250120).\n",
        "\n",
        "Note that DeepSeek offers a discount when using API in the midnight, details showing [here](https://api-docs.deepseek.com/zh-cn/quick_start/pricing)\n",
        "\n",
        "Below are the models OpenAI offers:\n",
        "\n",
        "- gpt-4o-mini (the most cost-effective model, highly recommended!)\n",
        "- gpt-4o (offers a balance of capability and cost)\n",
        "- gpt-4-turbo (extremely sophisticated and intelligent)\n",
        "\n",
        "For detailed information, please visit [OpenAI's Website](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "tNyoi8HSvZop",
        "outputId": "ca654479-2c83-44da-9c3c-60132d071817"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hello! I'm an AI assistant created by DeepSeek. You can call me Assistant or any name you like! üòä How can I help you today? And what's your name?\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "deepseek_reasoner = ChatDeepSeek(model=\"deepseek-reasoner\", temperature=1)\n",
        "chain = prompt | deepseek_reasoner | StrOutputParser()\n",
        "chain.invoke({'input': \"hello! what\\' your name?\"}) # Ê≥®ÊÑèreasonerÂæàË¥µÂì¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-Glb5kzzu1e"
      },
      "source": [
        "**2. Temperature**\n",
        "Controls the randomness of the model's output. Lower values make responses more deterministic and focused, while higher values allow for more creative and diverse outputs. Use low values for factual tasks and high values for creative tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VmnzLvyiw-aJ",
        "outputId": "46950dbc-26c6-40a7-bc2e-4b31d7e3d804"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hello! I'm an AI assistant created by DeepSeek. I don't have a personal name, but you can think of me as your helpful AI companion! How can I assist you today? üòä\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ChatDeepSeek(model=\"deepseek-chat\", temperature=0)\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke( {\"input\": 'hello! what\\' your name?'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp7Tlio5w_DB"
      },
      "source": [
        "**3. Top P**\n",
        "This is nucleus sampling, where only tokens from the top probability mass (up to `top_p`) are considered. Lower values encourage more focused responses, while higher values increase the diversity of possible outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "HShZxbyOu7b9",
        "outputId": "e4307ea2-bb22-4dac-ba58-00b408731898"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hello! I'm an AI assistant created by DeepSeek. I don't have a personal name, but you can just call me DeepSeek! I'm here to help answer your questions and assist with anything you need. What can I help you with today? üòä\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ChatDeepSeek(model=\"deepseek-chat\", top_p=0.9)\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke( {\"input\": 'hello! what\\' your name?'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW6oF6wK0L5B"
      },
      "source": [
        "**4. Max Length** This limits the total number of tokens the model can generate, helping control response length and prevent irrelevant output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o_cbFy0V0XmF",
        "outputId": "cdf755ba-3bd6-475a-ba6e-9374f5c95f07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hello! I'm an\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ChatDeepSeek(model=\"deepseek-chat\", max_tokens=5)\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke( {\"input\": 'hello! what\\' your name?'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOyoMZZKn9E"
      },
      "source": [
        "### **Prompting Techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW7OjIeqMzC3"
      },
      "source": [
        "**1. Zero-Shot Prompting** Large language models (LLMs) today, such as GPT-3.5 Turbo, GPT-4, and Claude 3, are tuned to follow instructions and are trained on large amounts of data. Large-scale training makes these models capable of performing some tasks in a \"zero-shot\" manner. Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h77_UeVAKnOy",
        "outputId": "6bdf2eec-1250-419b-dfc3-5c466a2ac09a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'neutral'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ChatDeepSeek(model=\"deepseek-chat\")\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "your_prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "Text: I think the vacation is okay.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "chain.invoke({\"input\": your_prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLUojJlxNHsn"
      },
      "source": [
        "**2. Few-Shot Prompting** While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response.\n",
        "\n",
        "According to [Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf) few shot properties first appeared when models were scaled to a sufficient size ([Kaplan et al., 2020](https://arxiv.org/abs/2001.08361))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aiMceIYUNISX",
        "outputId": "786d01ee-cf73-4c34-adaf-27c5717267bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Negative'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "your_prompt = \"\"\"This is bad! // Negative\n",
        "This is awesome! // Positive\n",
        "Wow that movie was rad! // Positive\n",
        "What a horrible show! //\"\"\"\n",
        "chain.invoke({\"input\": your_prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vah7gl7qNIlx"
      },
      "source": [
        "\n",
        "\n",
        "**3. Chain-of-Thought Prompting** Introduced in [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n",
        "\n",
        "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "cd0llsdSNJDC",
        "outputId": "b59707b5-d662-4af0-a01c-588a57fc8832"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Let‚Äôs break it down:  \\n\\n1. Start: 10 apples.  \\n2. Gave 2 to the neighbor: \\\\( 10 - 2 = 8 \\\\) apples left.  \\n3. Gave 2 to the repairman: \\\\( 8 - 2 = 6 \\\\) apples left.  \\n4. Bought 5 more: \\\\( 6 + 5 = 11 \\\\) apples.  \\n5. Ate 1: \\\\( 11 - 1 = 10 \\\\) apples.  \\n\\nSo, you remained with **10 apples**.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "your_prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "Let's think step by step.\"\"\"\n",
        "chain.invoke({\"input\": your_prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGKBvMJPOIoe"
      },
      "source": [
        "**4. Self-Consistency** Perhaps one of the more advanced techniques out there for prompt engineering is self-consistency. Proposed by [Wang et al. (2022)](https://arxiv.org/abs/2203.11171), self-consistency aims \"to replace the naive greedy decoding used in chain-of-thought prompting\". The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer. This helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "dz3RT22jOOzC",
        "outputId": "b348daf9-f8c2-42fa-8f9a-234a4ad435e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Chain of Thought:**\n",
            "\n",
            "1. **Cost of notebooks:**  \n",
            "   - 3 notebooks √ó $2 each = $6.\n",
            "\n",
            "2. **Cost of pens:**  \n",
            "   - 2 pens √ó $1.50 each = $3.\n",
            "\n",
            "3. **Total cost:**  \n",
            "   - $6 + $3 = $9.\n",
            "\n",
            "---\n",
            "\n",
            "**Multiple Reasoning Paths:**\n",
            "\n",
            "- **Path 1:**  \n",
            "  - Notebooks: 3 √ó $2 = $6  \n",
            "  - Pens: 2 √ó $1.50 = $3  \n",
            "  - Total: $6 + $3 = $9\n",
            "\n",
            "- **Path 2:**  \n",
            "  - Pens: 2 √ó $1.50 = $3  \n",
            "  - Notebooks: 3 √ó $2 = $6  \n",
            "  - Total: $3 + $6 = $9\n",
            "\n",
            "- **Path 3:**  \n",
            "  - Direct calculation:  \n",
            "    (3 √ó $2) + (2 √ó $1.50) = $6 + $3 = $9\n",
            "\n",
            "---\n",
            "\n",
            "**Consistency Check:**  \n",
            "All three reasoning paths yield the same result: **$9**.\n",
            "\n",
            "---\n",
            "\n",
            "**Final Answer:**  \n",
            "The total cost is **$9**.\n"
          ]
        }
      ],
      "source": [
        "your_prompt = \"\"\"**Problem: Calculate the total cost if you buy 3 notebooks at $2 each and 2 pens at $1.50 each.**\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "1. **Example Problem:** How many apples can you buy with $10 if each costs $2?\n",
        "   **Solution:**\n",
        "   - I have $10.\n",
        "   - Each apple costs $2.\n",
        "   - $10 / $2 = 5 apples.\n",
        "   **Answer: You can buy 5 apples.**\n",
        "\n",
        "2. **Example Problem:** If a train travels 50 miles in an hour, how far will it travel in 4 hours?\n",
        "   **Solution:**\n",
        "   - The train travels 50 miles in one hour.\n",
        "   - In 4 hours, it will travel 50 miles/hour * 4 hours = 200 miles.\n",
        "   **Answer: The train will travel 200 miles.**\n",
        "\n",
        "**Your Task:**\n",
        "\n",
        "- Use the chain of thought to break down the cost calculation.\n",
        "- Sample multiple reasoning paths.\n",
        "- Determine the most consistent calculation across different samples.\n",
        "\n",
        "**Reasoning:**\n",
        "- Start by identifying the cost of one category of items:\n",
        "  - 3 notebooks at $2 each = $6.\n",
        "- Then, calculate the cost for the other category:\n",
        "  - 2 pens at $1.50 each = $3.\n",
        "- Add both amounts to find the total cost:\n",
        "  - $6 (notebooks) + $3 (pens) = $9.\n",
        "\n",
        "**Consistency Check:**\n",
        "- Sample several reasoning paths. For example:\n",
        "  1. Calculate total cost for notebooks first, then pens, and sum.\n",
        "  2. Calculate total cost for pens first, then notebooks, and sum.\n",
        "  3. Directly multiply and add the costs of notebooks and pens.\n",
        "- Compare the answers and select the most frequently occurring result.\n",
        "\n",
        "**Final Answer:**\n",
        "- After verifying consistency across samples, conclude with the most consistent answer.\n",
        "\"\"\"\n",
        "chain.invoke({\"input\": your_prompt})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdwRvqrGOYJZ"
      },
      "source": [
        "\n",
        "\n",
        "**5. Tree of Thoughts (ToT)** For complex tasks that require exploration or strategic lookahead, traditional or simple prompting techniques fall short. [Yao et el. (2023)](https://arxiv.org/abs/2305.10601) and [Long (2023)](https://arxiv.org/abs/2305.08291) recently proposed Tree of Thoughts (ToT), a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that serve as intermediate steps for general problem solving with language models.\n",
        "\n",
        "ToT maintains a tree of thoughts, where thoughts represent coherent language sequences that serve as intermediate steps toward solving a problem. This approach enables an LM to self-evaluate the progress through intermediate thoughts made towards solving a problem through a deliberate reasoning process. The LM's ability to generate and evaluate thoughts is then combined with search algorithms (e.g., breadth-first search and depth-first search) to enable systematic exploration of thoughts with lookahead and backtracking.\n",
        "\n",
        "\n",
        "\n",
        "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e.png&w=3840&q=75)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "w32M94fOOYqE",
        "outputId": "943421a6-a4c9-42dd-dc97-4f3c8bbe85b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Let‚Äôs break this down step by step.  \\n\\n**Step 1: List possible first moves**  \\nI don‚Äôt have the actual puzzle rules or objects, so I‚Äôll make up a simple example to illustrate the method.  \\n\\nSuppose the objects are **A, B, C, D, E, F** and the rules are:  \\n1. A must come before B.  \\n2. C must come before D.  \\n3. E must come before F.  \\n4. B must come before D.  \\n5. No other constraints.  \\n\\nPossible first moves:  \\n- A can be first (no rule says something must be before A).  \\n- C can be first.  \\n- E can be first.  \\nBut B cannot be first (A must be before B), D cannot be first (C and B before D), F cannot be first (E before F).  \\n\\nSo possible first objects: **A, C, E**.  \\n\\n---\\n\\n**Step 2: Generate multiple intermediate thoughts for the second position**  \\n\\nLet‚Äôs pick **A** as first.  \\nRemaining objects: B, C, D, E, F.  \\n\\nPossible second positions:  \\n- **B** (A before B is satisfied).  \\n- **C** (no conflict).  \\n- **E** (no conflict).  \\nNot D yet (B must be before D, and B not placed yet ‚Üí but C must be before D too, so D can‚Äôt come until C and B are placed? Let‚Äôs check: Actually, C must be before D, but C can be placed later; B must be before D, but B can be placed later. So D can‚Äôt be second because C is not yet placed? Wait, that‚Äôs wrong ‚Äî C could be placed after D if we violate rule 2. So D can‚Äôt be placed until C is placed. So D cannot be second unless C is first, but C is not first here. So D can‚Äôt be second.)  \\n\\nSo possible second moves after A: B, C, E.  \\n\\n---\\n\\n**Step 3: Evaluate each option**  \\n\\nLet‚Äôs explore **A, B** as first two:  \\nRemaining: C, D, E, F.  \\nConstraints: C before D, E before F, B before D (B is placed, so D can only come after C is placed).  \\n\\nPossible third positions:  \\n- C (ok)  \\n- E (ok)  \\n- D? No, because C not yet placed.  \\n- F? No, because E not yet placed.  \\n\\nSo from A, B: possible third moves: C, E.  \\n\\nLet‚Äôs explore **A, C** as first two:  \\nRemaining: B, D, E, F.  \\nConstraints: A before B (ok to place B later), C before D (ok to place D later), E before F, B before D.  \\nPossible third moves: B, E, D? D is possible only if B is placed already? Wait, B before D means B must be placed before D. So if we try D third, B is not yet placed ‚Üí invalid. So D not allowed yet. So possible: B, E.  \\n\\nSimilarly, **A, E** as first two:  \\nRemaining: B, C, D, F.  \\nConstraints: E before F means F can‚Äôt come until E placed (E is placed, so F possible later).  \\nPossible third moves: B, C, F? F possible? Yes, E is placed, so F can come now. But check others: B before D still pending, C before D still pending. So possible: B, C, F.  \\n\\n---\\n\\n**Step 4: Search using BFS**  \\n\\nWe keep these partial sequences in a queue:  \\n\\nLevel 1: [A], [C], [E]  \\n\\nExpand [A]:  \\nChildren: [A,B], [A,C], [A,E]  \\n\\nExpand [C]:  \\nChildren: [C,A], [C,E], [C,?] ‚Äî Wait, from C first: possible second moves: A (ok), E (ok), B? B is ok (no rule says B must be after something except A, but A not placed yet ‚Üí so B before A? That‚Äôs allowed? A before B is required, so if B comes before A, that‚Äôs invalid. So B cannot be placed before A. So from C first, possible second moves: A, E, (B not allowed), D not allowed (B before D not satisfied yet? Actually B before D is separate; but C before D is satisfied if D placed now? But B before D is not satisfied if B not placed. So D can‚Äôt come until B placed. So D not allowed yet.) So possible: A, E.  \\n\\nSimilarly [E] first: possible second moves: A, C, B? B not allowed (A before B violated if A not placed), F? F allowed (E before F satisfied). So possible: A, C, F.  \\n\\nWe see how BFS expands multiple branches.  \\n\\n---\\n\\n**Step 5: Look ahead and refine**  \\n\\nLet‚Äôs take one branch: [A,B,C]  \\nRemaining: D, E, F.  \\nConstraints: C before D (ok), B before D (ok), E before F.  \\nPossible fourth moves: D, E.  \\n\\nIf D fourth: [A,B,C,D] ‚Üí remaining E,F ‚Üí must place E before F ‚Üí only order E,F. So sequence: A,B,C,D,E,F ‚Äî valid.  \\n\\nIf E fourth: [A,B,C,E] ‚Üí remaining D,F ‚Üí D must come after C (ok) and after B (ok), so D can be fifth, F sixth: A,B,C,E,D,F ‚Äî valid.  \\n\\nSo two complete sequences found already in this branch.  \\n\\n---\\n\\n**Step 6: Complete the arrangement**  \\n\\nWe‚Äôd continue for all branches until all valid sequences are found.  \\n\\n**Reasoning summary**:  \\n- Start with possible first objects by checking prerequisites.  \\n- Expand step by step, checking all constraints at each placement.  \\n- Use BFS to explore breadth and DFS deeply when a branch seems promising.  \\n- Backtrack when a dead end is reached (e.g., placing D before C or before B).  \\n- Multiple solutions possible unless additional constraints given.  \\n\\nIn this example, valid sequences include:  \\nA,B,C,D,E,F  \\nA,B,C,E,D,F  \\nA,C,B,D,E,F  \\nA,C,B,E,D,F  \\nA,C,E,B,D,F  \\nA,E,C,B,D,F  \\nC,A,B,D,E,F  \\nC,A,E,B,D,F  \\nE,A,C,B,D,F  \\netc. (depending on E,F order too).  \\n\\nThis systematic approach ensures we find all valid permutations satisfying the rules.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "your_prompt = \"\"\"Imagine you are solving a complex logic puzzle where you need to arrange six objects in a specific order based on a set of rules. Each object can only be placed once, and certain objects must be placed before others according to the rules provided below. You need to explore different arrangements systematically to find the correct solution. Follow these steps:_\n",
        "\n",
        "1. **Step 1: Start by listing possible first moves**: Consider each of the six objects and think through which ones could logically come first based on the rules.\n",
        "2. **Step 2: Generate multiple intermediate thoughts for the second position**: After selecting a first object, think about which objects can go next while considering the constraints. Explore at least three different possibilities.\n",
        "3. **Step 3: Evaluate each option**: After placing the first two objects, evaluate whether the current arrangement aligns with the rules. If it does, proceed to explore the third position. If not, backtrack and try another path.\n",
        "4. **Step 4: Search using Breadth-First Search (BFS)**: Expand on each potential arrangement one step at a time. Use BFS to keep track of multiple arrangements at once, evaluating their adherence to the rules as you go.\n",
        "5. **Step 5: Look ahead and refine**: After placing three objects, look ahead to the remaining positions and evaluate potential placements. If a placement leads to a conflict, backtrack and explore a different arrangement. Use depth-first search (DFS) if necessary to explore more deeply.\n",
        "6. **Step 6: Complete the arrangement and find the correct solution**: Continue exploring arrangements, self-evaluating each step, until the correct order is found. Summarize your reasoning process after completing the task.\"\"\"\n",
        "chain.invoke({\"input\": your_prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpmoWsqnP_Eo"
      },
      "source": [
        "**6. Retrieval Augmented Generation (RAG)** General-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don't require additional background knowledge.\n",
        "\n",
        "For more complex and knowledge-intensive tasks, it's possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of \"hallucination\".\n",
        "\n",
        "Meta AI researchers introduced a method called [Retrieval Augmented Generation (RAG)](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.\n",
        "\n",
        "\n",
        "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Frag.c6528d99.png&w=1920&q=75)  \n",
        "\n",
        "A recommended repository of implementations: (langchain) [https://github.com/langchain-ai/langchain]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "qEGZnErERUKd",
        "outputId": "a363f015-74cd-41f8-a8a1-ec5ce0de7d31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the retrieval content provided, the primary reasons for the decline in polar bear populations are:\\n\\n1. Ice melting\\n2. Loss of habitat\\n3. Changes in prey availability\\n\\nThese factors are all consequences of climate change affecting the Arctic region.\\n\\nTo mitigate this issue, comprehensive measures addressing climate change would be necessary, including:\\n- Reducing greenhouse gas emissions globally\\n- Protecting critical polar bear habitats\\n- Implementing conservation programs\\n- Supporting international agreements on climate action\\n- Monitoring polar bear populations and their ecosystems\\n\\nThe effectiveness of these measures depends on coordinated global efforts to address the root causes of climate change while implementing targeted conservation strategies in the Arctic region.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "your_prompt = \"\"\"## Instruction: Use the provided retrieval content to answer the question.\n",
        "### Retrieval Content:\n",
        "1. The retrieved document discusses the impact of climate change on polar bear populations in the Arctic, detailing factors like ice melting, loss of habitat, and changes in prey availability.\n",
        "\n",
        "### Question:\n",
        "What are the primary reasons for the decline in polar bear populations as discussed in the retrieval content, and what measures can be implemented to mitigate this issue?\n",
        "\"\"\"\n",
        "chain.invoke({\"input\": your_prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USvPTrsGST8S"
      },
      "source": [
        "**7. Automatic Reasoning and Tool-use (ART)** Combining CoT prompting and tools in an interleaved manner has shown to be a strong and robust approach to address many tasks with LLMs. These approaches typically require hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. Paranjape et al., (2023) propose a new framework that uses a frozen LLM to automatically generate intermediate reasoning steps as a program.\n",
        "\n",
        "ART works as follows:\n",
        "\n",
        "- given a new task, it select demonstrations of multi-step reasoning and tool use from a task library\n",
        "- at test time, it pauses generation whenever external tools are called, and integrate their output before resuming generation\n",
        "\n",
        "ART encourages the model to generalize from demonstrations to decompose a new task and use tools in appropriate places, in a zero-shot fashion. In addition, ART is extensible as it also enables humans to fix mistakes in the reasoning steps or add new tools by simply updating the task and tool libraries. The process is demonstrated below:\n",
        "\n",
        "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FART.3b30f615.png&w=1200&q=75)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCGdiS8SqtJ-"
      },
      "source": [
        "## **Task 2: LLMS for AI Feedback**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTLaP9TULpNS"
      },
      "source": [
        "The final stage of large language model training involves reinforcement learning through feedback. Such feedback can come from either  human experts or   AI. This feedback is used to learn a reward model, with data defined in a triplet form. This triplet comes from a question, two answers, and a choice by a human or AI on which answer is better.\n",
        "\n",
        "The triplet consists of three elements: a  *question*, the  *chosen* answer, and the *rejected* answer. You are asked to use ChatGPT to provide the feedback, namely, choose the preferred one. Note that the feedback is highly biased by the order of placed answers, please shuffle the order of answers when using chatGPT for preference feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget  https://NLP-course-cuhksz.github.io/Assignments/Assignment1/task2/data/1.rlhf.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chUVTLQLrFcF",
        "outputId": "564a8976-bafd-43ca-94d9-dd718c9cf552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Question': 'Human: What are some aspects of religious Islam that are compatible with western civilization?',\n",
              " 'Answer1': \"Assistant: I'm not sure what 'western civilization' means in this context, but if we assume it means the values of modern Western culture, it seems there are many facets of Islamic religious practice that are compatible with those values.  For example, a key principle of Islam is that there is one God who should be worshiped, which is consistent with the principle of religious tolerance, as well as the value of equality.  There are different branches of Islam, but many of them require\",\n",
              " 'Answer2': 'Assistant: Well in a basic sense it is possible for Muslims to live in the Western world and take part in Western civilization. In fact, many Muslims have been at the center of important cultural developments of the last centuries.',\n",
              " 'Preference': 'Answer2'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "with open('1.rlhf.jsonl') as f:\n",
        "  data = [json.loads(l) for l in f]\n",
        "data[0] # sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTPm1-ngWQ38",
        "outputId": "dfa0ed7a-54f3-452f-e252-ef0b47caea9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Question': 'Human: What are some aspects of religious Islam that are compatible with western civilization?',\n",
              " 'Answer1': \"Assistant: I'm not sure what 'western civilization' means in this context, but if we assume it means the values of modern Western culture, it seems there are many facets of Islamic religious practice that are compatible with those values.  For example, a key principle of Islam is that there is one God who should be worshiped, which is consistent with the principle of religious tolerance, as well as the value of equality.  There are different branches of Islam, but many of them require\",\n",
              " 'Answer2': 'Assistant: Well in a basic sense it is possible for Muslims to live in the Western world and take part in Western civilization. In fact, many Muslims have been at the center of important cultural developments of the last centuries.',\n",
              " 'Preference': 'Answer2'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nci7lKr6rdT8",
        "outputId": "a7d4d7f6-443c-42b2-8b4a-b42bb0b6f5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Question]:\n",
            "Human: What are some aspects of religious Islam that are compatible with western civilization?\n",
            "\n",
            "[Answer1]:\n",
            "Assistant: I'm not sure what 'western civilization' means in this context, but if we assume it means the values of modern Western culture, it seems there are many facets of Islamic religious practice that are compatible with those values.  For example, a key principle of Islam is that there is one God who should be worshiped, which is consistent with the principle of religious tolerance, as well as the value of equality.  There are different branches of Islam, but many of them require\n",
            "\n",
            "[Answer2]:\n",
            "Assistant: Well in a basic sense it is possible for Muslims to live in the Western world and take part in Western civilization. In fact, many Muslims have been at the center of important cultural developments of the last centuries.\n",
            "\n",
            "A good response should be generally helpful, accurate, correct, and safe.\n",
            "Choose which answer is overall better. Output only 'Answer1' or 'Answer2'.\n",
            "\n",
            "---- GPT-4o response ----\n",
            "Answer1\n"
          ]
        }
      ],
      "source": [
        "your_prompt = '''\n",
        "[Question]:\n",
        "{Question}\n",
        "\n",
        "[Answer1]:\n",
        "{Answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{Answer2}\n",
        "\n",
        "A good response should be generally helpful, accurate, correct, and safe.\n",
        "Choose which answer is overall better. Output only 'Answer1' or 'Answer2'.\n",
        "'''\n",
        "\n",
        "def get_query(da):\n",
        "  return your_prompt.format_map(da)\n",
        "\n",
        "testdata = data[0]\n",
        "\n",
        "print(get_query(testdata))\n",
        "\n",
        "print(f'---- GPT-4o response ----')\n",
        "\n",
        "print(chain.invoke(get_query(testdata)).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeHYMM_rr7aV",
        "outputId": "78e13e31-0e54-4187-ed0e-92cae1a12421"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:36<00:00,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model consistency rate with human: 67.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "your_prompt = '''\n",
        "[Question]:\n",
        "{Question}\n",
        "\n",
        "[Answer1]:\n",
        "{Answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{Answer2}\n",
        "\n",
        "A good response should be generally helpful, accurate, correct, and safe.\n",
        "Choose which answer is overall better. Output only 'Answer1' or 'Answer2'.\n",
        "'''\n",
        "\n",
        "def get_query(da):\n",
        "  return your_prompt.format_map(da)\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "\n",
        "# Test on the entire dataset\n",
        "for da in tqdm(data):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if da['deepseek_ans'].content == da['Preference']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"Model consistency rate with human: {correct_num/total_num:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_nVCyGLFn6U"
      },
      "source": [
        "<font color=\"blue\">You need to optimize the prompt to improve the performance (consistency rate) of large language models (LLMs).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Few Shot (Entire Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:35<00:00,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model consistency rate with human: 68.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "your_prompt = '''\n",
        "\n",
        "==== Demonstrations ====\n",
        "[Question]:\n",
        "What is the common side effect of aspirin?\n",
        "\n",
        "[Answer1]:\n",
        "Aspirin may cause stomach irritation and bleeding.\n",
        "\n",
        "[Answer2]:\n",
        "Aspirin is a vitamin supplement.\n",
        "\n",
        "[Better]:\n",
        "Answer1\n",
        "\n",
        "\n",
        "[Question]:\n",
        "What is the purpose of antibiotics?\n",
        "\n",
        "[Answer1]:\n",
        "They kill or inhibit bacterial growth.\n",
        "\n",
        "[Answer2]:\n",
        "They relieve joint pain.\n",
        "\n",
        "[Better]:\n",
        "Answer1\n",
        "\n",
        "\n",
        "[Question]:\n",
        "Where is the capital of France?\n",
        "\n",
        "[Answer1]:\n",
        "The capital of France is Paris.\n",
        "\n",
        "[Answer2]:\n",
        "France is in Europe.\n",
        "\n",
        "[Better]:\n",
        "Answer1\n",
        "\n",
        "\n",
        "==== Evaluate ====\n",
        "[Question]:\n",
        "{Question}\n",
        "\n",
        "[Answer1]:\n",
        "{Answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{Answer2}\n",
        "\n",
        "A good response should be generally helpful, accurate, correct, and safe.\n",
        "Choose which answer is overall better. Output only 'Answer1' or 'Answer2'.\n",
        "'''\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "for da in tqdm(data):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if da['deepseek_ans'].content == da['Preference']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"Model consistency rate with human: {correct_num/total_num:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CoT + Few Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:02<00:00,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model consistency rate with human: 60.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "your_prompt = '''\n",
        "\n",
        "==== Demonstrations ====\n",
        "[Question]:\n",
        "What is the common side effect of aspirin?\n",
        "\n",
        "[Answer1]:\n",
        "Aspirin may cause stomach irritation and bleeding.\n",
        "\n",
        "[Answer2]:\n",
        "Aspirin is a vitamin supplement.\n",
        "\n",
        "[Better]:\n",
        "Answer1\n",
        "\n",
        "\n",
        "[Question]:\n",
        "What is the purpose of antibiotics?\n",
        "\n",
        "[Answer1]:\n",
        "They kill or inhibit bacterial growth.\n",
        "\n",
        "[Answer2]:\n",
        "They relieve joint pain.\n",
        "\n",
        "[Better]:\n",
        "Answer1\n",
        "\n",
        "\n",
        "[Question]:\n",
        "Where is the capital of France?\n",
        "\n",
        "[Answer1]:\n",
        "The capital of France is Paris.\n",
        "\n",
        "[Answer2]:\n",
        "France is in Europe.\n",
        "\n",
        "[Better]:\n",
        "Answer1\n",
        "\n",
        "\n",
        "==== Evaluate ====\n",
        "[Question]:\n",
        "{Question}\n",
        "\n",
        "[Answer1]:\n",
        "{Answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{Answer2}\n",
        "\n",
        "A good response should be generally helpful, accurate, correct, and safe.\n",
        "Choose which answer is overall better. Output only 'Answer1' or 'Answer2'.\n",
        "\n",
        "Let's think step-by-step\n",
        "'''\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "for da in tqdm(data):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if da['deepseek_ans'].content == da['Preference']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"Model consistency rate with human: {correct_num/total_num:.2%}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CoT + Zero Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:01<00:00,  1.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model consistency rate with human: 63.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "your_prompt = '''\n",
        "[Question]:\n",
        "{Question}\n",
        "\n",
        "[Answer1]:\n",
        "{Answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{Answer2}\n",
        "\n",
        "A good response should be generally helpful, accurate, correct, and safe.\n",
        "Choose which answer is overall better. Output only 'Answer1' or 'Answer2'.\n",
        "\n",
        "Let's think step-by-step\n",
        "'''\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "\n",
        "\n",
        "for da in tqdm(data):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if da['deepseek_ans'].content == da['Preference']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"Model consistency rate with human: {correct_num/total_num:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model_4o_mini = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini-2024-07-18\",\n",
        "    temperature=1,\n",
        "    openai_api_key=\"sk-\",\n",
        "    openai_api_base=\"http://66.206.9.230:4000/v1\"   # explicit argument\n",
        ")\n",
        "\n",
        "chain = prompt | model_4o_mini \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Self-consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:40<00:00,  1.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model consistency rate with human: 70.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "your_prompt = \"\"\"**Problem: Decide which answer better satisfies the user's question.**\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "1. **Example Question:** What is the common side effect of aspirin?\n",
        "   **Answer1:** Aspirin may cause stomach irritation and bleeding.\n",
        "   **Answer2:** Aspirin is a vitamin supplement.\n",
        "   **Better:** Answer1\n",
        "\n",
        "2. **Example Question:** What is the purpose of antibiotics?\n",
        "   **Answer1:** They relieve joint pain.\n",
        "   **Answer2:** They kill or inhibit bacterial growth.\n",
        "   **Better:** Answer2\n",
        "\n",
        "3. **Example Question:** Where is the capital of France?\n",
        "   **Answer1:** The capital of France is Paris.\n",
        "   **Answer2:** France is in Europe.\n",
        "   **Better:** Answer1\n",
        "\n",
        "\n",
        "**Evaluate:**\n",
        "[Question]:\n",
        "{Question}\n",
        "\n",
        "[Answer1]:\n",
        "{Answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{Answer2}\n",
        "\n",
        "**Your Task:**\n",
        "- You are an impartial judge for a *single* triplet (Question, Answer1, Answer2) from a preference dataset.\n",
        "- Your goal is to choose which answer better satisfies the question.\n",
        "- Judge by: relevance to the question, factual accuracy, helpfulness/clarity, and safety (avoid harmful or misleading content).\n",
        "\n",
        "**Reasoning (do this silently):**\n",
        "- Think step-by-step *internally* (do not reveal your chain of thought).\n",
        "- Consider whether each answer is on-topic, correct, and sufficiently informative.\n",
        "- Prefer precise, directly responsive, and safe content.\n",
        "- If both are weak, choose the *less* harmful/misleading and more relevant one.\n",
        "\n",
        "**Consistency Check (do this silently):**\n",
        "- Internally sample multiple reasoning paths and compare your conclusions.\n",
        "- If any paths disagree, reconcile them and choose the answer supported by the strongest consistent reasoning.\n",
        "- Do not output your reasoning‚Äîonly the final label.\n",
        "\n",
        "\n",
        "\n",
        "**Output Format (STRICT):**\n",
        "Output exactly one token on a single line: Answer1 or Answer2.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "\n",
        "\n",
        "for da in tqdm(data):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if da['deepseek_ans'].content == da['Preference']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"Model consistency rate with human: {correct_num/total_num:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change to another model - Gpt 4 Turbo\n",
        "\n",
        "However, the accuracy score decreased, possibly due to noise / distractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model_4o = ChatOpenAI(\n",
        "    model=\"gpt-4-turbo-2024-04-09\",\n",
        "    temperature=1,\n",
        "    openai_api_key=\"sk-\",\n",
        "    openai_api_base=\"http://66.206.9.230:4000/v1\"   # explicit argument\n",
        ")\n",
        "\n",
        "chain = prompt | model_4o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:04<00:00,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model consistency rate with human: 67.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "your_prompt = \"\"\"**Problem: Decide which answer better satisfies the user's question.**\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "1. **Example Question:** What is the common side effect of aspirin?\n",
        "   **Answer1:** Aspirin may cause stomach irritation and bleeding.\n",
        "   **Answer2:** Aspirin is a vitamin supplement.\n",
        "   **Better:** Answer1\n",
        "\n",
        "2. **Example Question:** What is the purpose of antibiotics?\n",
        "   **Answer1:** They relieve joint pain.\n",
        "   **Answer2:** They kill or inhibit bacterial growth.\n",
        "   **Better:** Answer2\n",
        "\n",
        "3. **Example Question:** Where is the capital of France?\n",
        "   **Answer1:** The capital of France is Paris.\n",
        "   **Answer2:** France is in Europe.\n",
        "   **Better:** Answer1\n",
        "\n",
        "\n",
        "**Evaluate:**\n",
        "[Question]:\n",
        "{Question}\n",
        "\n",
        "[Answer1]:\n",
        "{Answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{Answer2}\n",
        "\n",
        "**Your Task:**\n",
        "- You are an impartial judge for a *single* triplet (Question, Answer1, Answer2) from a preference dataset.\n",
        "- Your goal is to choose which answer better satisfies the question.\n",
        "- Judge by: relevance to the question, factual accuracy, helpfulness/clarity, and safety (avoid harmful or misleading content).\n",
        "\n",
        "**Reasoning (do this silently):**\n",
        "- Think step-by-step *internally* (do not reveal your chain of thought).\n",
        "- Consider whether each answer is on-topic, correct, and sufficiently informative.\n",
        "- Prefer precise, directly responsive, and safe content.\n",
        "- If both are weak, choose the *less* harmful/misleading and more relevant one.\n",
        "\n",
        "**Consistency Check (do this silently):**\n",
        "- Internally sample multiple reasoning paths and compare your conclusions.\n",
        "- If any paths disagree, reconcile them and choose the answer supported by the strongest consistent reasoning.\n",
        "- Do not output your reasoning‚Äîonly the final label.\n",
        "\n",
        "\n",
        "\n",
        "**Output Format (STRICT):**\n",
        "Output exactly one token on a single line: Answer1 or Answer2.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "\n",
        "\n",
        "for da in tqdm(data):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if da['deepseek_ans'].content == da['Preference']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"Model consistency rate with human: {correct_num/total_num:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [11:37<00:00,  6.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model consistency rate with human: 66.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# pip install langchain-community wikipedia\n",
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "wiki = WikipediaRetriever(top_k_results=2)\n",
        "\n",
        "def rag_judge(question, answer1, answer2, llm, temperature=0.0):\n",
        "    \"\"\"Retrieve Wikipedia evidence and decide which answer is better.\"\"\"\n",
        "    try:\n",
        "        docs = wiki.invoke(question)[:2]\n",
        "    except Exception:\n",
        "        docs = []\n",
        "    evidence = \"\\n\".join(\n",
        "        f\"[E{i+1}] {getattr(d, 'page_content','').strip().replace('\\n',' ')}\"\n",
        "        for i, d in enumerate(docs)\n",
        "        if getattr(d, \"page_content\", \"\").strip()\n",
        "    ) or \"No evidence found.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "==== Evidence ====\n",
        "{evidence}\n",
        "\n",
        "==== Evaluate ====\n",
        "[Question]:\n",
        "{question}\n",
        "\n",
        "[Answer1]:\n",
        "{answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{answer2}\n",
        "\n",
        "Rules:\n",
        "- Prefer the answer that best matches the Evidence (accuracy first).\n",
        "- Also consider relevance, clarity, and safety.\n",
        "- If evidence is unclear, choose using your own reasoning.\n",
        "Output exactly one line:\n",
        "Final: Answer1\n",
        "or\n",
        "Final: Answer2\n",
        "\"\"\".strip()\n",
        "\n",
        "    try:\n",
        "        resp = llm.invoke({\"input\": prompt}, temperature=temperature)\n",
        "    except TypeError:\n",
        "        resp = llm.invoke({\"input\": prompt})\n",
        "    text = getattr(resp, \"content\", str(resp)).strip()\n",
        "    m = re.search(r\"\\bFinal\\s*:\\s*(Answer1|Answer2)\\b\", text, re.I)\n",
        "    return m.group(1).title() if m else None\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Evaluation loop\n",
        "# =========================================================\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "\n",
        "for da in tqdm(data):\n",
        "    pred = rag_judge(da[\"Question\"], da[\"Answer1\"], da[\"Answer2\"], chain, temperature=0.0)\n",
        "    da[\"deepseek_ans\"] = pred\n",
        "    if pred == da[\"Preference\"]:\n",
        "        correct_num += 1\n",
        "    total_num += 1\n",
        "\n",
        "print(f\"Model consistency rate with human: {correct_num/total_num:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Self Consistency - but adjust Temperature and Top P for better score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:51<00:00,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model consistency rate with human: 74.00%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "your_prompt = \"\"\"**Problem: Decide which answer better satisfies the user's question.**\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "1. **Example Question:** What is the common side effect of aspirin?\n",
        "   **Answer1:** Aspirin may cause stomach irritation and bleeding.\n",
        "   **Answer2:** Aspirin is a vitamin supplement.\n",
        "   **Better:** Answer1\n",
        "\n",
        "2. **Example Question:** What is the purpose of antibiotics?\n",
        "   **Answer1:** They relieve joint pain.\n",
        "   **Answer2:** They kill or inhibit bacterial growth.\n",
        "   **Better:** Answer2\n",
        "\n",
        "3. **Example Question:** Where is the capital of France?\n",
        "   **Answer1:** The capital of France is Paris.\n",
        "   **Answer2:** France is in Europe.\n",
        "   **Better:** Answer1\n",
        "\n",
        "\n",
        "**Evaluate:**\n",
        "[Question]:\n",
        "{Question}\n",
        "\n",
        "[Answer1]:\n",
        "{Answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{Answer2}\n",
        "\n",
        "**Your Task:**\n",
        "- You are an impartial judge for a *single* triplet (Question, Answer1, Answer2) from a preference dataset.\n",
        "- Your goal is to choose which answer better satisfies the question.\n",
        "- Judge by: relevance to the question, factual accuracy, helpfulness/clarity, and safety (avoid harmful or misleading content).\n",
        "\n",
        "**Reasoning (do this silently):**\n",
        "- Think step-by-step *internally* (do not reveal your chain of thought).\n",
        "- Consider whether each answer is on-topic, correct, and sufficiently informative.\n",
        "- Prefer precise, directly responsive, and safe content.\n",
        "- If both are weak, choose the *less* harmful/misleading and more relevant one.\n",
        "\n",
        "**Consistency Check (do this silently):**\n",
        "- Internally sample multiple reasoning paths and compare your conclusions.\n",
        "- If any paths disagree, reconcile them and choose the answer supported by the strongest consistent reasoning.\n",
        "- Do not output your reasoning‚Äîonly the final label.\n",
        "\n",
        "\n",
        "\n",
        "**Output Format (STRICT):**\n",
        "Output exactly one token on a single line: Answer1 or Answer2.\n",
        "\"\"\"\n",
        "\n",
        "temperature = 0.2   # 0 = deterministic, 0.3‚Äì0.7 = some diversity\n",
        "top_p = 0.9         # 1.0 = full sampling; try 0.9 for narrower focus\n",
        "\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "\n",
        "\n",
        "for da in tqdm(data):\n",
        "  da['deepseek_ans'] = chain.invoke(\n",
        "        get_query(da),\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "    )\n",
        "\n",
        "  if da['deepseek_ans'].content == da['Preference']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"Model consistency rate with human: {correct_num/total_num:.2%}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
